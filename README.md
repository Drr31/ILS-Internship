# ILS-Internship

> **PRIVATE & CONFIDENTIAL â€” Please do not share or distribute without permission**

---

## Overview

This repository contains the complete work done during my internship on **extending and scaling coverage metrics for Neural Network testing**, based on the CoVerNet methodology. The project focuses on improving the testing and verification processes of a neural network.

---

## Structure

The repo is organized by weekly progress, with dedicated notebooks and scripts that document the theory, experiments, and results:

- **Week 1:** Foundations and theoretical preparation, including a deep dive into neural networks and refreshing essential math concepts.

- **Week 2:** Exploration and application of Python testing frameworks (`unittest`, `pytest`, `doctest`) to verify components of the pipeline.

- **Week 3:** Clustering techniques and their application on datasets (e.g., Iris) for equivalence class formation.

- **Week 4:** Implementation and timing analysis of Prior-Equivalence Classes (PEC) generation as part of the CoVerNet approach.

---

## Key Topics Covered

- Neural network fundamentals and coverage concepts  
- Clustering algorithms (KMeans) and data scaling impact  
- Software testing practices for machine learning code  
- Performance evaluation and scalability analysis  
- Formal coverage metrics and equivalence class generation  

---

## Usage

The notebooks contain explanations, code, and visualizations to help understand each step. To reproduce the results or build upon the work:

1. Clone this repository (private access required)  
2. Install dependencies (e.g., `numpy`, `scikit-learn`, `matplotlib`, `pandas`)  
3. Open the weekly notebooks in Jupyter and follow along with the code and comments  

---

## Confidentiality Notice

This repository contains confidential work done during an internship and should not be shared outside authorized parties. Unauthorized use or distribution is prohibited.

---
